<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PCA主成分分析</title>
      <link href="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
      <url>/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>PCA就是<strong>降维</strong>保存信息，例如将二维数据保留为一维数据。就是找到一个新的坐标系，这个坐标系的原点落在数据中心，坐标系的方向是往数据分布的方向走。我们需要保存的是新坐标系的原点、新坐标系的角度和新的坐标点。</p><p>PCA的目标就是要找到坐标系，使得保留了某些维度的时候，<strong>信息损失是最小的(信息保留最多)<strong>，比如说投影在某个轴上面，</strong>数据分布是最分散的</strong>。我们要找到数据分布最分散的方向(方差最大)，作为主成分(坐标轴)。新坐标系的第一个维度称为主成分一，第二个维度称为主成分二，如果我们找到数据在主成分一上面的投影分布方差是最大的时候，那么说明主成分一它能够保留最多的信息，在这个时候的坐标系就是最好的坐标系。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%9D%90%E6%A0%87%E8%BD%B4%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="坐标轴示意图"></p><p>如何实现PCA？</p><p>1.首先是要去中心化，就是把坐标原点放在数据的中心。方法就是把每一个值减去全部值的平均值。移动数据并不会改变数据点彼此之间的相对位置。</p><p>2.然后就是找坐标系(找到方差最大的方向) </p><p>我们该如何找到方差最大的方向呢？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2.png" alt="数据线性变换"></p><p>如果只是数据对角矩阵。用待变换矩阵左乘它就会得到<strong>伸缩</strong>之后的坐标，如图所示就是将x轴拉伸2倍长。原来是单位矩阵左乘，那么是让坐标不变，现在相应的扩大几倍。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A22.png" alt="数据线性变换2"></p><p>在矩阵运算上面可以想象到，这是逆时针旋转相应度数</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E8%BD%AC%E6%8D%A2.png" alt="转换"></p><p><strong>我们手上的数据就是我们要降维的数据</strong>。可以通过白数据左乘S再左乘R得到。</p><p>拉伸和旋转变换有什么作用？</p><p>拉伸决定了<strong>方差最大的方向</strong>是横或者纵向。旋转决定了方差最大的方向的角度是多大。<strong>所以我们要求的就是R矩阵</strong>。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E8%BD%AC%E6%8D%A22.png" alt="转换2"></p><p><strong>我们手上的数据</strong>D’同样也可以转换回白数据。可以先乘一个R的逆矩阵，它本来是逆时针旋转，取逆就是再顺时针旋转同样的度数。再拉伸，压缩回拉伸的倒数。</p><p>怎么求R？<strong>协方差矩阵的特征向量就是R</strong>。</p><p>何为协方差矩阵？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt="什么是协方差"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt="协方差矩阵"></p><p>白数据x,y不相关，这个时候协方差为0，只剩下对角线，即x、y方差为1就是一个单位矩阵。数据正相关协方差大于0，数据负相关，协方差小于0。</p><p>将协方差的公式代入协方差矩阵得到：再提个1&#x2F;(N-1)</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%BB%A3%E5%85%A5%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt="代入协方差矩阵"></p><p>这里的D是移至原点后的<strong>原始数据矩阵</strong></p><p>我的想法是C既拉伸又旋转？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%89%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt="手上数据的协方差"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F.png" alt="协方差的特征向量"></p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/R和L矩阵.png" alt="R和L矩阵" style="zoom: 80%;"><p>把特征值放到一起组成L矩阵，把特征向量v1和v2组合到一起成为R</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%80%BC.png" alt="协方差的特征值"></p><p>旋转回来协方差是L，因为R逆是一个对角矩阵，旋转回来之后x方向和y方向不相关，x方向的方差是a²，y方向的方差是b²，同时又是协方差矩阵的特征值。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3PCA.png" alt="如何求解PCA"></p><p>如何判断PCA拟合度的高低？我们可以通过量化数据在主成分上的投影长度和最小或者数据在投影点到中心点的距离和最大，通常我们选择后者，因为后者更方便。</p><p>计算每个主成分的差异率就是把主成分分别的特征值除以全部的特征值加起来，越大越好，代表信息越多</p><p>主成分分析的应用</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%B8%A4%E7%BB%84%E6%95%B0%E6%8D%AE.png" alt="两组数据"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%A0%B7%E6%9C%AC%E7%82%B9%E5%8E%BB%E4%B8%AD%E5%BF%83%E5%8C%96.png" alt="样本点去中心化"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E7%BB%93%E6%9E%9C.png" alt="结果"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E7%BB%98%E5%9B%BE.png" alt="绘图"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8E%9F%E6%9C%89%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8C%96%E4%B8%BA%E4%B8%80%E7%BB%B4%E6%95%B0%E6%8D%AE.png" alt="原有数据转化为一维数据"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/PCA%E6%AD%A5%E9%AA%A4.png" alt="PCA步骤"></p><p>主成分分析的本质就是向量换基。</p><p>主成分分析通过对投影距离方差的运用将降维问题转换成了求最值的问题。</p><p>PCA还有其他的实现方法，例如使用相关矩阵代替协方差矩阵，也可以使用奇异值分解(SVD)，但是原理类似。</p><p>如果让输入数据矩阵维度在行上，观测值在列上，推导可能会更简洁一些(特征向量不用转置)，但是不影响结果。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Action</title>
      <link href="/2022/12/30/Action/"/>
      <url>/2022/12/30/Action/</url>
      
        <content type="html"><![CDATA[<h2 id="上传一个文件到博客的步骤如下："><a href="#上传一个文件到博客的步骤如下：" class="headerlink" title="上传一个文件到博客的步骤如下："></a>上传一个文件到博客的步骤如下：</h2><h3 id="清理缓存"><a href="#清理缓存" class="headerlink" title="清理缓存"></a>清理缓存</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><h3 id="生成新的静态文件"><a href="#生成新的静态文件" class="headerlink" title="生成新的静态文件"></a>生成新的静态文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><h3 id="本地预览博客"><a href="#本地预览博客" class="headerlink" title="本地预览博客"></a>本地预览博客</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><h3 id="上传到github仓库，将文件部署到博客上"><a href="#上传到github仓库，将文件部署到博客上" class="headerlink" title="上传到github仓库，将文件部署到博客上"></a>上传到github仓库，将文件部署到博客上</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><h2 id="其他的一些设置："><a href="#其他的一些设置：" class="headerlink" title="其他的一些设置："></a>其他的一些设置：</h2><h3 id="生成新的md文件"><a href="#生成新的md文件" class="headerlink" title="生成新的md文件"></a>生成新的md文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo n</span><br></pre></td></tr></table></figure><h2 id="butterfly设置："><a href="#butterfly设置：" class="headerlink" title="butterfly设置："></a>butterfly设置：</h2><h3 id="创建一个新的标签页"><a href="#创建一个新的标签页" class="headerlink" title="创建一个新的标签页"></a>创建一个新的标签页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page tags</span><br></pre></td></tr></table></figure><h3 id="创建一个分类页"><a href="#创建一个分类页" class="headerlink" title="创建一个分类页"></a>创建一个分类页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page categories</span><br></pre></td></tr></table></figure><h3 id="创建友情链接"><a href="#创建友情链接" class="headerlink" title="创建友情链接"></a>创建友情链接</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page <span class="built_in">link</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 技术操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/2022/12/30/CNN/"/>
      <url>/2022/12/30/CNN/</url>
      
        <content type="html"><![CDATA[<p><strong>卷积神经网络用处在哪？</strong></p><p>卷积神经网络是用于特征提取的。</p><p>传统的神经网络权重参数矩阵大、训练时间多、过拟合风险高。</p><p>可用于检测追踪任务、分类和检索、超分辨率重构、无人驾驶、人脸识别等</p><img src="/2022/12/30/CNN/超分辨率.png" alt="超分辨率" style="zoom:50%;"><p><strong>卷积神经网络(CNN)与传统网络(NN)的区别：</strong></p><img src="/2022/12/30/CNN/卷积神经网络与传统网络的区别.png" alt="卷积神经网络与传统网络的区别" style="zoom: 80%;"><ul><li>NN输入的是像素点，而CNN输入的是一张原始图像(h×w×c)，一个是一维，一个是三维。</li></ul><p><strong>卷积神经网络的整体架构：</strong></p><ul><li>输入层：输入一个图像数据</li><li>卷积层：尽可能多的提取特征</li><li>池化层：压缩、下采样特征</li><li>全连接层：通过一组权重参数，把输入层和输出层连在一起</li></ul><p><strong>卷积：</strong>卷积做了一件什么事？</p><img src="/2022/12/30/CNN/卷积做了一件什么事.png" alt="卷积做了一件什么事" style="zoom:60%;"><p>对不同的区域提取出不同的特征，将一张图像分成不同的部分，区别处理，进行图像分割。</p><p><strong>图像颜色通道</strong>：</p><p>R,G,B：要对三个颜色通道分别做计算，把三个通道卷积完的结果<strong>加</strong>在一起。对于每一个区域都要进行特征提取，得到最终的特征值。</p><img src="/2022/12/30/CNN/RGB特征提取.png" alt="RGB特征提取" style="zoom:67%;"><p>输入的图像维度c是多少，那么卷积核的维度c也应该是多少。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积核就是：每多大区域选出一个特征，一个区域对应出一个特征值。 相当于权重，刚开始是随机初始化的，然后学习更新。即下图W0</p><p>所有的卷积网络都是用<strong>内积</strong>做计算，<strong>对应位置相乘</strong>，所有结果加一起就可以了。结果别忘了加一个偏置项。在每一个卷积层中的特征矩阵w,h应该是相同的。在不通的卷积层中w,h可以不同</p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B.png" alt="卷积过程"></p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B2.png" alt="卷积过程2"></p><p><strong>卷积层涉及的参数</strong>：</p><ul><li><p>滑动窗口步长：注意<strong>步长</strong>为2得到的最终结果才是3*3，步长小，提取特征比较细致，效率慢；步长大，提取特征比较粗糙，特征少。<strong>一般对于图像而言，我们选择步长为1就可以</strong>，但是对于文本数据和其他数据步长不确定。</p></li><li><p>卷积核尺寸：卷积核越小，特征提取越细致，一般来说选<strong>3*3</strong></p></li><li><p>边缘填充：在边界外再加几圈0，能够弥补一些边界信息利用不充分问题。最外层只是扩充，因为是0，所以对最终结果不会产生影响。一般添一圈</p></li><li><p>卷积核个数：在算的过程当中要得到多少个特征图就有多少个卷积核。</p></li></ul><p>特征图的个数：特征图的个数取决于你给了多少份的权重矩阵，选择不通的权重矩阵，得到的特征图个数结果不一样。</p><p>卷积神经网络不止可以做一次卷积，一次可以提取出粗略特征，再卷积一次可以提取出中间特征，最后再提取出高级特征，再拿出高级特征来做分类。做一次卷积是不够的，需要做多次。</p><p><strong>卷积结果计算公式：</strong></p><img src="/2022/12/30/CNN/卷积结果计算公式.png" alt="卷积结果计算公式" style="zoom:80%;"><img src="/2022/12/30/CNN/卷积结果计算例子.png" alt="卷积结果计算例子" style="zoom: 33%;"><p><strong>卷积参数共享：</strong></p><p> 对于图中的每个区域都选择同样的卷积核，卷积核这个权值矩阵是不变的</p><p><img src="/2022/12/30/CNN/%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB.png" alt="参数共享"></p><p>每次卷积完都要加一个RELU函数，即非线性变换</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>在原始得到的特征上进行一个筛选，并不会改变特征图的个数。不涉及矩阵计算，只涉及筛选。</p><p><strong>最大池化</strong>：</p><p>选择不同区域，在每个区域中选择最大值，选择一个最大值说明这个特征比较重要。</p><img src="/2022/12/30/CNN/最大池化.png" alt="最大池化" style="zoom: 67%;"><p><strong>平均池化</strong>：</p><p>选择不同区域，在每个区域中求平均值。</p><p>我们<strong>选择最大池化</strong>，因为神经网络是个优胜劣汰的过程，我们选择最好的特征，不平均来把不好的特征拷进去。</p><p>两次卷积后一次池化，RELU是激活函数</p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E6%B1%A0%E5%8C%96.png" alt="卷积池化"></p><p>卷积和池化只是做特征提取的，到最后池化后会形成立体的特征图，对特征图进行分类，如何转化为分类的概率值？全连接层无法连三维的东西，我们需要将三维的特征图拉长形成特征向量，全连接层如果是五分类，池化层得到的特征图大小为<code>32*32*10</code>，那么得到的全连接层是[10240,5]，相当于将10240个特征转化为我们预测的五个类别的概率值。所以在Pooling层和FC层之间还有一个拉长的操作（转换操作）</p><p><strong>什么才能称为一层</strong>？带参数计算的才能被称为一层。卷积层带，RELU层(激活层)不带参数计算，池化层不带参数计算，不更新参数之类的，全连接层也有权重参数矩阵，需要更新参数。</p><img src="/2022/12/30/CNN/特征图的变化.png" alt="特征图的变化" style="zoom:67%;"><p><strong>感受野：</strong></p><img src="/2022/12/30/CNN/感受野.png" alt="感受野" style="zoom:60%;"><p>我们希望感受野越大越好。</p><img src="/2022/12/30/CNN/感受野2.png" alt="感受野2" style="zoom:60%;">]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学建模 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

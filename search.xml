<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>层次分析法AHP</title>
      <link href="/2023/01/16/AHP%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"/>
      <url>/2023/01/16/AHP%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>首先，我需要说明的是，层次分析法相对于主成分分析、因子分析、熵权法等，主观性太强了，没有客观性，在评价类模型中历史悠久，相对也简单，如果不会其他模型，拿来混是没有问题的，或者说在一个题目我用两个评价模型，其中有一个是层次分析法，也没啥问题，但是单独拿来用，对于提高自己的奖项不是很有利，所以不建议用。但是因为有学习过，所以我还是把笔记做好，供SPSSPRO之层次分析法(简化版)使用。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 评价类模型 </tag>
            
            <tag> 不建议使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>层次分析法AHP</title>
      <link href="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"/>
      <url>/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>首先，我需要说明的是，层次分析法相对于主成分分析、因子分析、熵权法等，主观性太强了，没有客观性，在评价类模型中历史悠久，相对也简单，如果不会其他模型，拿来混是没有问题的，或者说在一个题目我用两个评价模型，其中有一个是层次分析法，也没啥问题，但是单独拿来用，对于提高自己的奖项不是很有利，所以不建议用。但是因为有学习过，所以我还是把笔记做好，供SPSSPRO之层次分析法(简化版)使用。</p><p>综合评价例题引入：</p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/例题引入.png" alt="例题引入" style="zoom:80%;"><p>目标是要根据评价结果进行排名，这是一个<strong>评价类</strong>的问题。</p><p><strong>目标</strong>是选出第一名的UP，所以目标层即第一名；四条可量化的评价指标，就是<strong>准则层</strong>的四个准则；<strong>方案层</strong>是五个UP，即所有可能的结果。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt="模型结构"></p><p>我们应该从下往上看，从方案层的方案个数，通过方案之间的共性得到准则层，通过准则层求出目标层。所有连线代表所有的方案都需要进行评价给出得分。</p><p><strong>如何根据四项指标对五位UP进行综合评价？</strong></p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4%E6%B3%95.png" alt="简单粗暴法"></p><p>归一化处理是减少数量级的，要记得是在同一个数量级进行归一化，在这里就体现为对粉丝数、播放量、获赞数、稿件数分别进行归一化，不要弄错归一化是行还是列。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90.png" alt="问题分析"></p><p>层次分析法的缺点就是主观性太强了，需要自己设计权重，并且需要科学合理设计权重，不能<strong>有逻辑上的错误</strong>就有如下的判断矩阵，判断矩阵的目的就是为了求出权重：</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E5%88%A4%E6%96%AD%E7%9F%A9%E9%98%B5%E5%BC%95%E5%85%A5.png" alt="判断矩阵引入"></p><p>其中，目标层就是评出第一，准则层有四条，即粉丝数、播放数…</p><p>例子中粉丝数与稿件数相比为5：1，那么稿件数与粉丝数相比就是1：5，所以是倒数关系。</p><p>记住这个判断矩阵是根据我们的主观想法得到的。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E7%9F%9B%E7%9B%BE%E5%A4%84.png" alt="矛盾处"></p><p>在我们填写完整判断矩阵后，会出现图片中的不一致现象，这是逻辑上的矛盾。出现矛盾的原因是我们两两比较时并不会考虑其他因素的存在，忽略了其他因素和本我们要比较的两个因素之间的逻辑关系。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A3%80%E9%AA%8C%E5%BC%95%E5%85%A5.png" alt="一致性检验引入"></p><p>如果我们设立的判断矩阵矩阵值满足理想情况的式子，我们就说这个矩阵是一致矩阵，一致矩阵是一种十分理想的情况，实际中我们并不要求矛盾一定是不出现的，而是出现矛盾的次数少。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/CR%E7%9A%84%E5%85%B7%E4%BD%93%E6%B1%82%E6%B3%95.png" alt="CR的具体求法"></p><p>我们实际操作中判断矩阵的一致性比例CR一定要＜0.1，否则我们需要手动修改判断矩阵使得CR＜0.1</p><p>一致性比例代表的意思就是说，我们手动设置的矩阵和一致矩阵的差异有多大。</p><p>指标数n对应的RI是固定的，需要通过查表得到。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E4%BF%AE%E6%94%B9%E5%88%A4%E6%96%AD%E7%9F%A9%E9%98%B5.png" alt="修改判断矩阵"></p><p>CR&#x3D;0.042说明此时还存在微小矛盾，但矛盾已经不影响做题了</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E6%B1%82%E6%9D%83%E9%87%8D.png" alt="求权重"></p><p>我们对判断矩阵进行按列归一化而不进行按行归一化，原因是按列归一化得到的才是某一指标相对的重要性，不明白的话，记住就好。</p><p>我们使用的是算术平均法求权重得到的结果A权重最大，说明A比其他的指标更重要，在判断矩阵中也能说明。</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E7%BB%93%E6%9E%9C.png" alt="结果"></p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95%E6%A8%A1%E5%9E%8B.png" alt="层次分析法模型"></p><p>读完题发现是评价类问题，就考虑能不能用层次分析法，于是确定目标层、准则层、方案层，就可以进行一致性检验…</p><p><img src="/2023/01/16/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/%E9%A2%98%E7%9B%AE%E4%B8%AD%E6%9B%B4%E6%99%AE%E9%81%8D%E7%9A%84%E6%83%85%E5%86%B5.png" alt="题目中更普遍的情况"></p><p>如果准则层没有客观数据怎么办？也就是没有准则的具体数据，只有权重怎么办？</p><p>我们就要按照求准则层相对于目标层的判断矩阵的方法，同样求出方案层对于准则层的判断矩阵。我们求的是四个评价指标对于第一名这个评价结果的权重。现在求例如求影响力就是五位UP对影响力，也是一样的，我们两两比较构造判断矩阵，然后求出权重向量，每个指标都有一个权重向量，所以就会有一个5*4的矩阵，同样的方法。</p><p>还有一种情况就是准则层是两层而不是单层该怎么办？等竞赛遇到了再仔细研究。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 评价类模型 </tag>
            
            <tag> 不建议使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>因子分析</title>
      <link href="/2023/01/16/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"/>
      <url>/2023/01/16/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 评价类模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门</title>
      <link href="/2023/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
      <url>/2023/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="python一些入门"><a href="#python一些入门" class="headerlink" title="python一些入门"></a>python一些入门</h1><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>导入：<code>import numpy as np</code>，将numpy作为np导入</p><p>生成Numpy数组(numpy.ndarray)：使用<code>np.array()</code>方法。<code>np.array()</code>以列表为参数。它<strong>和一维数组是不同的</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;x = np.array([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>])</span><br><span class="line">&gt;&gt;&gt;<span class="built_in">print</span>(x)</span><br><span class="line">[ <span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(x)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br></pre></td></tr></table></figure><p>Numpy数组的算术运算：对应位置的运算，数组元素个数要相同。</p><p>Numpy生成二维数组：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(A)</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line">[<span class="number">3</span> <span class="number">4</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.shape <span class="comment"># 查看矩阵A的形状</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.dtype <span class="comment"># 查看矩阵元素的类型</span></span><br><span class="line">dtype(<span class="string">&#x27;int64&#x27;</span>)</span><br></pre></td></tr></table></figure><p>数学上将一维数组称为向量，将二维数组称为矩阵。可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为“矩阵”，将三维数组及三维以上的数组称为“张量”或“多维数组”。</p><p>NumPy 有广播功能，所以<strong>不同形状的数组</strong>之间也可以顺利地进行运算。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = np.array([<span class="number">10</span>, <span class="number">20</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * B</span><br><span class="line">array([[ <span class="number">10</span>, <span class="number">40</span>],</span><br><span class="line">   [ <span class="number">30</span>, <span class="number">80</span>]])</span><br></pre></td></tr></table></figure><p><code>flatten()</code>函数，可以将Numpy数组转化为一维数组</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[<span class="number">51</span>, <span class="number">55</span>], [<span class="number">14</span>, <span class="number">19</span>], [<span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = X.flatten() <span class="comment"># 将 X 转换为一维数组</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(X)</span><br><span class="line">[<span class="number">51</span> <span class="number">55</span> <span class="number">14</span> <span class="number">19</span> <span class="number">0</span> <span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>])] <span class="comment"># 获取索引为 0、2、4 的元素</span></span><br><span class="line">array([<span class="number">51</span>, <span class="number">14</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>运用这个标记法，可以获取满足一定条件的元素。例如，要从 X 中抽出大于 15 的元素，可以写成如下形式：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X &gt; <span class="number">15</span></span><br><span class="line">array([ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>], dtype=<span class="built_in">bool</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[X&gt;<span class="number">15</span>]</span><br><span class="line">array([<span class="number">51</span>, <span class="number">55</span>, <span class="number">19</span>])</span><br></pre></td></tr></table></figure><p>对 NumPy 数组使用不等号运算符等（上例中是 X &gt; 15）, 结果会得到一个布尔型的数组。上例中就是使用这个布尔型数组取出了数组的各个元素（取出 True 对应的元素）。</p><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>用于绘制图形和实现数据的可视化。可以使用 matplotlib 的 pyplot 模块绘制图形。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 以 0.1 为单位，生成 0 到 6 的数据</span></span><br><span class="line">y = np.sin(x)</span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这里使用 NumPy 的 arange 方法生成了 [0, 0.1, 0.2, …, 5.8, 5.9]的数据，将其设为 x。对 x 的各个元素，应用 NumPy 的 sin 函数 np.sin()，将 x、y 的数据传给 plt.plot 方法，然后绘制图形。最后，通过 plt.show() 显示图形。显示结果如下</p><img src="/2023/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/Figure_1.png" alt="Figure_1" style="zoom:67%;"><p><strong>pyplot的功能：</strong></p><p>在刚才的 sin 函数的图形中，我们尝试追加 cos 函数的图形，并尝试使用<br>pyplot 的添加标题和 x 轴标签名等其他功能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 以 0.1 为单位，生成 0 到 6 的数据</span></span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">plt.plot(x, y1, label=<span class="string">&quot;sin&quot;</span>)</span><br><span class="line">plt.plot(x, y2, linestyle = <span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;cos&quot;</span>) <span class="comment"># 用虚线绘制</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>) <span class="comment"># x 轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>) <span class="comment"># y 轴标签</span></span><br><span class="line">plt.title(<span class="string">&#x27;sin &amp; cos&#x27;</span>) <span class="comment"># 标题</span></span><br><span class="line">plt.legend() <span class="comment"># 图例</span></span><br><span class="line">plt.show() <span class="comment">#显示</span></span><br></pre></td></tr></table></figure><p>结果如下：</p><img src="/2023/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/Figure_2.png" alt="Figure_2" style="zoom:72%;"><p><strong>显示图像：</strong></p><p>pyplot中还提供了用于显示图像的方法<code>imshow()</code>，另外可以使用matplotlib.image模块的<code>imread()</code>方法读入图像。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"></span><br><span class="line">img = imread(<span class="string">&#x27;./img/dog.jpg&#x27;</span>) <span class="comment"># 读入图像（设定合适的路径！）</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下：<img src="/2023/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/Figure_3.png" alt="Figure_3"></p><h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch</title>
      <link href="/2023/01/09/pytorch/"/>
      <url>/2023/01/09/pytorch/</url>
      
        <content type="html"><![CDATA[<p>创建新项目的时候选择的编译器要选择Conda环境，如果手选Conda环境，要在Anaconda3的envs这个文件里选pytorch里面的python.exe就可以了。</p><h1 id="两个重要函数"><a href="#两个重要函数" class="headerlink" title="两个重要函数"></a>两个重要函数</h1><p>pytorch是一个package工具箱</p><ul><li><code>dir()</code>:打开，看见里面有什么东西。当使用dir()得到的参数是双下划线的时候，说明这些标识符是函数。</li><li><code>help()</code>：说明书，还有一种形式例如<code>Dataset??</code><ul><li>例如查看<code>is_available()</code>函数，我们就运行<code>help(torch.cuda.is_available)</code>，括号里只需要输入标识符即可</li></ul></li></ul><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><p>如何读取数据涉及到两个类</p><ul><li>Dataset：提供一种方式去获取数据及其label。<ul><li>Dataset是一个抽象类，所有数据集都要继承它，所有子类都要实现<code>getitem</code>方法，选择性重写<code>len</code>方法。</li><li>如何获取每一个数据及其label？实现<code>getitem</code>方法。</li><li>告诉我们总共有多少的数据？重写<code>len</code>方法</li></ul></li><li>Dataloader：对我们要送进网络的数据进行打包，为网络提供不同的数据形式</li></ul><p><strong>import os</strong>:</p><ul><li><p><code>os.path.join(str1,str2...)</code>函数的参数可以放字符串，得到的字符串结果是把多个字符串用<code>\\</code>连接，因为单独的\会被当成转义字符，&#x2F;并不是转义字符，例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">root_dir=<span class="string">&quot;dataset/train&quot;</span>label_dir=<span class="string">&quot;ants&quot;</span></span><br><span class="line">path=os.path.join(root_dir,label_dir)</span><br><span class="line">&gt;&gt;&gt;path=&#123;<span class="built_in">str</span>&#125;<span class="string">&#x27;dataset/train\\ants&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p><code>os.listdir(path)</code>函数可以将路径path中的文件转化成列表形式，例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_path_list=os.listdir(path)</span><br><span class="line">&gt;&gt;&gt;img_path_list=&#123;<span class="built_in">list</span>:<span class="number">124</span>&#125;[...]</span><br></pre></td></tr></table></figure></li></ul><p><strong>from PIL import Image</strong>：</p><ul><li><code>Image.open(img_path)</code>此函数用于创建图片，返回<code>&#123;JpegImageFile&#125;</code>变量</li><li><code>img.show()</code>函数用于显示图片</li></ul><p>对目前所学的一个例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 从常用工具区的关于数据的 data 区 import Dataset</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  <span class="comment">#读取图片</span></span><br><span class="line"><span class="keyword">import</span> os <span class="comment">#operating system 关于系统的库</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):<span class="comment">#继承Dataset类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,root_dir,label_dir</span>):</span><br><span class="line">        self.root_dir=root_dir  <span class="comment">#self相当于在类中创建了全局变量</span></span><br><span class="line">        self.label_dir=label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = self.img_path[idx]<span class="comment">#注意要加self</span></span><br><span class="line">        img_item_path = os.path.join(self.path,img_name)</span><br><span class="line">        img=Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = self.label_dir <span class="comment">#相对于本个例子而言的label</span></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    <span class="comment">#这里重写了getitem，返回img，label，就达到了我们利用下标获取每一个数据及其label的目的</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)<span class="comment">#返回列表长度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;dataset/train&quot;</span></span><br><span class="line"></span><br><span class="line">ants_label_dir = <span class="string">&quot;ants&quot;</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)</span><br><span class="line"></span><br><span class="line">bees_label_dir = <span class="string">&quot;bees&quot;</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ants_dataset[<span class="number">0</span>])<span class="comment">#会输出img以及对应的label</span></span><br><span class="line">img,label = ants_dataset[<span class="number">0</span>]<span class="comment">#实现了geititem函数，有两个返回值</span></span><br><span class="line">img.show()//显示图片</span><br><span class="line">img,label = bees_dataset[<span class="number">1</span>]</span><br><span class="line">img.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(ants_dataset)<span class="comment">#得到124</span></span><br><span class="line"><span class="built_in">len</span>(bees_dataset)<span class="comment">#得到121</span></span><br><span class="line">train_dataset = ants_dataset+bees_dataset<span class="comment">#这个数据集会变成ants_dataset和bees_dataset的拼接，达到仿造数据集，解决数据集不足问题</span></span><br><span class="line"><span class="built_in">len</span>(train_dataset)<span class="comment">#得到245</span></span><br><span class="line"></span><br><span class="line">img,label=train_dataset[<span class="number">123</span>]<span class="comment">#得到蚂蚁，蚂蚁是[0-123]</span></span><br><span class="line">img,label=train_dataset[<span class="number">124</span>]<span class="comment">#得到蜜蜂，蜜蜂是[124-244]</span></span><br></pre></td></tr></table></figure><p>作出数据集的另一种存储方式：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#把图片的label生成以图片名为文件名的txt文档</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&#x27;练手数据集/train&#x27;</span></span><br><span class="line">target_dir = <span class="string">&#x27;ants_image&#x27;</span></span><br><span class="line">img_path = os.listdir(os.path.join(root_dir, target_dir))</span><br><span class="line">label = target_dir.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">out_dir = <span class="string">&#x27;ants_label&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> img_path:</span><br><span class="line">    file_name = i.split(<span class="string">&#x27;.jpg&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(root_dir, out_dir,<span class="string">&quot;&#123;&#125;.txt&quot;</span>.<span class="built_in">format</span>(file_name)),<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:<span class="comment">#open会生成不存在的文件</span></span><br><span class="line">        f.write(label)</span><br></pre></td></tr></table></figure><h1 id="TensorBoard的使用"><a href="#TensorBoard的使用" class="headerlink" title="TensorBoard的使用"></a>TensorBoard的使用</h1><p>TensorBoard：演示transform的结果，在运行完方法后展示图像。例如loss图、训练结果output。</p><p>首先要<code>from torch.utils.tensorboard import SummaryWriter</code>。</p><h2 id="SummaryWriter类使用"><a href="#SummaryWriter类使用" class="headerlink" title="SummaryWriter类使用"></a>SummaryWriter类使用</h2><p><code>CTRL</code>+鼠标移至类可以查看类的用法。</p><ul><li><p>SummerWriter将条目直接写入 TensorBoard 要使用的<code>log_dir</code>中的事件文件，事件文件可以直接被TensorBoard解析。</p></li><li><p>初始化的时候需要输入<code>log_dir</code>文件夹的名称，不输入也可以，有默认位置。</p></li><li><p>初始化方式：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter()<span class="comment">#什么都不加，默认的事件文件存储的位置在runs/下</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;my_experiment&quot;</span>)<span class="comment">#给出初始化文件夹</span></span><br><span class="line">writer = SummaryWriter(comment=<span class="string">&quot;LR_0.1_BATCH_16&quot;</span>)<span class="comment">#设置相应参数</span></span><br></pre></td></tr></table></figure></li></ul><p>主要用到两个方法：</p><ul><li><code>writer.add_scalar(tag(string),scalar_value(float or string/blobname),global_step(int))</code><ul><li>参数<code>tag(string)</code>设置图表的title</li><li>参数<code>scalar_value(float or string/blobname)</code>设置图表y轴</li><li>参数<code>global_step(int)</code>对应x轴</li></ul></li></ul><p>打开事件文件，在pycharm的命令行，输入<code>tensorboard --logdir=事件文件所在文件夹名</code>。如果端口被占用了，可以自行添加端口，输入<code>tensorboard --logdir=事件文件所在文件夹名 --port=端口值  </code> </p><p>如果打开的tensorboard出现了重复在一张图上的情况，需要进行删除原来的事件文件，或者重新SummaryWriter一个<code>log_dir</code>文件</p><ul><li><code>writer.add_image(tag(string),img_tensor(torch.Tensor, numpy.array, or string/blobname),global_step(int))</code><ul><li>参数<code>tag(string)</code>，设置图像的title</li><li>参数<code>img_tensor(torch.Tensor, numpy.array, or string/blobname)</code>，设置图像</li><li>参数<code>global_step(int)</code>，设置训练步骤</li></ul></li></ul><p>可以利用Opencv读取图片，获得numpy型图片数据。</p><p>如果是用PIL数据转化成numpy数据，需要在add_image()中指定shape中每一个数字&#x2F;维表示的含义。主要是通道数。</p><p>step可以理解为多张图片可以放在同一个title里面，通过滑块可以看到每一步的图片，如果想让多张图片单独显示，需要把title重新命名</p><p>上面的代码综合运用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)<span class="comment">#存储文件夹</span></span><br><span class="line"><span class="comment"># y=3x</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>): <span class="comment">#i从0-99</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=3x&quot;</span>,<span class="number">3</span>*i,i)<span class="comment"># 添加标量</span></span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img_array = np.array(img)   <span class="comment"># 类型转换，这样得到的numpy.array的通道是在最后面的</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_array))</span><br><span class="line"><span class="built_in">print</span>(img_array.shape)      <span class="comment"># 结果是(512,768,3)</span></span><br><span class="line">writer.add_image(<span class="string">&quot;train&quot;</span>,img_array,<span class="number">1</span>,dataformats=<span class="string">&#x27;HWC&#x27;</span>)<span class="comment">#需要指定是HWC型，因为数据就是通道在最后面，不然会报错</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="Transforms的使用"><a href="#Transforms的使用" class="headerlink" title="Transforms的使用"></a>Transforms的使用</h1><p>Transforms主要是用于对图片进行一些变换。将特定格式的图片经过工具之后输出我们想要的图片结果。</p><h2 id="Transforms的结构及用法"><a href="#Transforms的结构及用法" class="headerlink" title="Transforms的结构及用法"></a>Transforms的结构及用法</h2><p>Transforms.py工具箱具有的工具：</p><ul><li>Totensor类，Totensor就是把PIL Image或者numpy.ndarray类型转化为tensor类型的</li><li>resize类，裁剪</li><li>…</li></ul><h2 id="tensor数据类型"><a href="#tensor数据类型" class="headerlink" title="tensor数据类型"></a>tensor数据类型</h2><p>通过transforms.Totensor去解决两个问题：</p><ul><li><p>transforms该如何使用(python)？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_trans = transforms.ToTensor() <span class="comment">#返回totensor的对象，默认构造</span></span><br><span class="line">tensor_img = tensor_trans(img)<span class="comment">#将img转化为tensor类型的img</span></span><br></pre></td></tr></table></figure></li><li><p>Tensor数据类型相较于普通的数据类型或者图片数据类型有何区别？为什么需要tensor的数据类型？</p><p>Tensor数据类型包装了神经网络所需要的一些参数</p></li></ul><p>上述示例代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="built_in">print</span>(img) <span class="comment"># 得到&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x512 at 0x1B1C3BB77F0&gt;</span></span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor() <span class="comment">#新建totensor的对象</span></span><br><span class="line">tensor_img = tensor_trans(img)<span class="comment">#调用call函数，调用类的名称时会自动运行该方法，将img转化为tensor类型的img</span></span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br><span class="line"></span><br><span class="line">cv_img = cv2.imread(img_path) <span class="comment">#cv_img的数据类型就是numpy.ndarray</span></span><br><span class="line">tensor_cv_img = tensor_trans(cv_img)<span class="comment">#将cv_img转化为tensor类型的img</span></span><br><span class="line"><span class="built_in">print</span>(tensor_cv_img)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>,tensor_img)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>python中call函数的作用：call函数可以直接使用<code>对象名(参数)</code>这种方式去调用，这种方式就像Java重写的toString一样</p><h2 id="常见的Transform"><a href="#常见的Transform" class="headerlink" title="常见的Transform"></a>常见的Transform</h2><p>Compose类：把不同的transform结合在一起</p><p>ToTensor类：把PIL Image或者numpy.ndarray类型转化为tensor类型</p><p>ToPILImage类：把tensor类型或者ndarray类型转化为PIL Image类型</p><p>Normalize类：用平均值和标准差对tensor image(图像张量)进行归一化，归一到[-1,1]</p><p>Resize类：把图像缩放成指定的大小，如果给了元组序列，图像就会按元组序列的大小来重新调整，如果只是给了一个int型，图像就会以这个int型进行等比缩放，最短边的长度等于int值。输入为PIL Image，返回值还是PIL Image</p><p>RandomCrop类：随即裁剪，和Resize用法差不多，输入都是 PIL Image</p><p>使用方法总结：1. 关注输入和输出类型，输出可以直接print()看，也可以断点看 2.多看官方文档 3.关注方法需要的参数，没有设置默认值就是要输入的参数，找到它然后查看它的类型</p><p>上述代码实例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;data/train/ants_image/0013035.jpg&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Totensor</span></span><br><span class="line">trans_totensor = transforms.ToTensor()</span><br><span class="line">img_tenser = trans_totensor(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;ToTenser&quot;</span>, img_tenser)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"><span class="comment"># 归一化计算公式：output[channel] = (input[channel] - mean[channel]) / std[channel]</span></span><br><span class="line"><span class="built_in">print</span>(img_tenser[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>])</span><br><span class="line">img_norm = trans_norm(img_tenser)</span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">writer.add_image(<span class="string">&quot;Normalize&quot;</span>,img_norm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line"><span class="built_in">print</span>(img.size) <span class="comment">#(768, 512)</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line"><span class="comment"># img是PIL类型，经过resize返回还是一个PIL类型</span></span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"><span class="built_in">print</span>(img_resize)   <span class="comment">#&lt;PIL.Image.Image image mode=RGB size=512x512 at 0x13442E71430&gt;</span></span><br><span class="line"><span class="comment">#img_resize.show()</span></span><br><span class="line"><span class="comment"># PIL类型使用totensor转化为tensor对象</span></span><br><span class="line">img_resize = trans_totensor(img_resize)</span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>,img_resize,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compose — resize的第二种用法</span></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>)</span><br><span class="line"><span class="comment"># PIL Image-&gt;PIL Image-&gt;tensor 要注意列表中前一个输出和后一个输入是不是相互匹配，因为是联系的</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2,trans_totensor])</span><br><span class="line">img_resize_2 = trans_compose(img)<span class="comment">#第一个transform需要的输入</span></span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>,img_resize_2,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomCrop</span></span><br><span class="line">trans_random = transforms.RandomCrop(<span class="number">512</span>)</span><br><span class="line">trans_compose_2 = transforms.Compose([trans_random,trans_totensor])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_crop = trans_compose_2(img)</span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>,img_crop,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="将dataset和transform结合到一起"><a href="#将dataset和transform结合到一起" class="headerlink" title="将dataset和transform结合到一起"></a>将dataset和transform结合到一起</h1><p>dataset告诉我们数据集在什么样的位置和多少数据，给索引＋数据</p><p>具体实例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 保存数据集的路径，训练集还是测试集，transform，下载到本地否，&#x27;.&#x27;表示当前目录,&#x27;..&#x27;表示上一级目录</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=dataset_transform,download=<span class="literal">True</span>)<span class="comment">#训练集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=dataset_transform,download=<span class="literal">True</span>)<span class="comment">#测试集</span></span><br><span class="line"><span class="comment"># (&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x2784D869610&gt;, 3),3是target</span></span><br><span class="line"><span class="comment"># 原始图片是PIL Image</span></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(test_set.classes)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line">img.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img,target = test_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&quot;test_set&quot;</span>,img,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h1><p>dataloader就是从dataset中取数据，参数控制取多少怎么取</p><ul><li>batch_size这个参数是指每次取多少数据</li><li>suffle这个参数是打乱顺序</li><li>num_workers多线程，加载数据的时候采用单个线程还是多个线程，默认情况为0，采用主线程</li><li>drop_last是否舍去最后的余数</li></ul><p>代码实例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment">#取出四个img，四个target的形式分别打包返回</span></span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=<span class="number">64</span>,shuffle=<span class="literal">False</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集中第一张图片及target</span></span><br><span class="line">img,target = test_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):<span class="comment">#相当于一轮结束的洗牌</span></span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        <span class="comment"># print(imgs.shape)</span></span><br><span class="line">        <span class="comment"># print(targets)</span></span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch),imgs,step)</span><br><span class="line">        step = step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="网络基本骨架的搭建"><a href="#网络基本骨架的搭建" class="headerlink" title="网络基本骨架的搭建"></a>网络基本骨架的搭建</h1><p>关于神经网络的工具一般都在torch.nn<Neural network>里面</Neural></p><p>Containers—&gt;神经网络的骨架、结构，常用Module模块，为所有神经网络提供一个基本的骨架，所搭建的神经网络必须从torch.nn.Module类继承而来。其中有个forward函数，它是call方法的实现，在所有继承了torch.nn.Module类的子类中都要重写，是前向传播，比较重要，类似于call方法直接调用类作为方法，它的作用是输入通过forward变为输出</p><p>Module的使用代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#这种方法也可以初始化，但是我不知道区别在哪</span></span><br><span class="line">    <span class="comment">#def __init__(self):</span></span><br><span class="line">        <span class="comment">#super(Model, self).__init__()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="built_in">input</span> +<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># 把x放到神经网络中</span></span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p>Sequential的目的可以让神经网络代码压缩在一段代码中，过程连续。</p><p>sequential实例代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        <span class="comment"># self.conv1 = Conv2d(3,32,5,padding=2)</span></span><br><span class="line">        <span class="comment"># self.maxpool1 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.conv2 = Conv2d(32,32,5,padding=2)</span></span><br><span class="line">        <span class="comment"># self.maxpool2 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.conv3 = Conv2d(32,64,5,padding=2)</span></span><br><span class="line">        <span class="comment"># self.maxpool3 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.flatten = Flatten()</span></span><br><span class="line">        <span class="comment"># self.linear1 = Linear(1024,64)</span></span><br><span class="line">        <span class="comment"># self.linear2 = Linear(64,10)</span></span><br><span class="line"></span><br><span class="line">        self.module1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"><span class="comment"># 检验网络参数是否正确的话，可以一步步写forward 然后运行查看output.shape</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># x = self.conv1(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool1(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv2(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool2(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv3(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool3(x)</span></span><br><span class="line">        <span class="comment"># x = self.flatten(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear1(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear2(x)</span></span><br><span class="line"></span><br><span class="line">        x = self.module1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="built_in">print</span>(cnn)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">output = cnn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">writer.add_graph(cnn,<span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="Covolution-Layers—-gt-卷积层"><a href="#Covolution-Layers—-gt-卷积层" class="headerlink" title="Covolution Layers—&gt;卷积层"></a>Covolution Layers—&gt;卷积层</h1><p>卷积核随机初始化，网络训练的就是卷积核</p><p>卷积操作内部具体实现&lt;实践用不到&gt;：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 数据类型转换</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment">#调整尺寸，由于cov2d的input需要四位张量</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="built_in">print</span>(kernel.shape)</span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"></span><br><span class="line">output3 = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output3)</span><br></pre></td></tr></table></figure><p>代码示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NN, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nn = NN()</span><br><span class="line"><span class="built_in">print</span>(nn)</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    output = nn(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    <span class="comment">#torch.Size([64, 3, 32, 32])</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    <span class="comment">#torch.Size([64, 6, 30, 30]) 会报错，因为彩色图像为3个channel，6个channel不知道怎么显示</span></span><br><span class="line"></span><br><span class="line">    output = torch.reshape(output,(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">30</span>,<span class="number">30</span>))<span class="comment">#设置-1，会根据数据自动调整</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="Pooling-Layers—-gt-池化层"><a href="#Pooling-Layers—-gt-池化层" class="headerlink" title="Pooling Layers—&gt;池化层"></a>Pooling Layers—&gt;池化层</h1><p>最大池化目的：保留输入的特征，同时把参数量减少</p><p>代码示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                     [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)<span class="comment">#其中数字转化为浮点数，编程浮点数的tensor类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">maxpool</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(maxpool, self).__init__()</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mp = maxpool()</span><br><span class="line"><span class="comment"># output = mp(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    output = mp(imgs)<span class="comment">#得到的图像依然是三维的</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h1><p>RELU，当input＞0时，赋值为input原来值，当input＜0时，赋值为0，输入只有N—&gt;batch_size；Sigmoid，也只是输入N。</p><p>ReLU的实例代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,-<span class="number">0.5</span>],</span><br><span class="line">                     [-<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RELU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(RELU, self).__init__()</span><br><span class="line">        self.relu = ReLU()<span class="comment">#inplace这个参数如果是True就在原值上进行改变，如果为False原值不变，返回改变后的值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output = self.relu(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">relu = RELU()</span><br><span class="line">output = relu(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p>Sigmoid的实例代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU, Sigmoid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SigMoid</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SigMoid, self).__init__()</span><br><span class="line">        self.sigmoid = Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output = self.sigmoid(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">sigmoid = SigMoid()</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset,batch_size = <span class="number">64</span>)</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    output=sigmoid(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>Padding Layers—&gt;填充层，用的不多，在卷积里能达到同样的效果</p><p>Normalization Layers—&gt;标准化层，用的不多</p><h1 id="Linear-Layers—-gt-全连接层"><a href="#Linear-Layers—-gt-全连接层" class="headerlink" title="Linear Layers—&gt;全连接层"></a>Linear Layers—&gt;全连接层</h1><p>Linear代码示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset =torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader= DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LINEAR</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LINEAR, self).__init__()</span><br><span class="line">        self.linear = Linear(<span class="number">196608</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output = self.linear(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">linear = LINEAR()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,target = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># output = torch.reshape(imgs,(1,1,1,-1))</span></span><br><span class="line">    output = torch.flatten(imgs)<span class="comment">#扁平化处理</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    output = linear(output)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p>Loss Function：loss就是输出值与实际的误差值，越小越好，是神经网络训练的依据。可以计算实际输出和目标之间的差距，为我们更新输出提供一定的依据(反向传播)。</p><ul><li>CrossEntropyLoss，交叉熵，分类问题中有C个类别。</li></ul><p>各种loss函数的使用</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss, MSELoss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实际默认的数据就是float，这是因为我们创建tensor的时候没有加小数</span></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss = L1Loss()<span class="comment">#默认是取平均差值，可以选择取加值</span></span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result)<span class="comment">#tensor(0.6667)</span></span><br><span class="line"></span><br><span class="line">loss_mse = MSELoss()<span class="comment">#取平均平方差</span></span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result_mse)<span class="comment">#tensor(1.3333)</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>))<span class="comment">#一张图，三个分类</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train = <span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.module1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.module1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment">#看outputs和targets长什么样，看选择什么样的损失函数</span></span><br><span class="line">    imgs,targets = data</span><br><span class="line">    output = cnn(imgs)</span><br><span class="line">    <span class="comment"># print(output)</span></span><br><span class="line">    <span class="comment"># print(targets)</span></span><br><span class="line">    <span class="comment"># 反向传播，求导，需要选择合适的优化器</span></span><br><span class="line">    result_loss = loss(output,targets)</span><br><span class="line">    <span class="comment"># print(result_loss)</span></span><br><span class="line">    <span class="comment">#得到反向传播要更新参数对应的梯度</span></span><br><span class="line">    result_loss.backward()</span><br></pre></td></tr></table></figure><p>加上参数调优之后的整个网络：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train = <span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.module1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.module1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="comment"># 优化器,第一个参数是模型的参数</span></span><br><span class="line">optim = torch.optim.SGD(cnn.parameters(),lr= <span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="comment"># 看outputs和targets长什么样，看选择什么样的损失函数</span></span><br><span class="line">        imgs,targets = data</span><br><span class="line">        output = cnn(imgs)</span><br><span class="line">        <span class="comment"># 计算loss</span></span><br><span class="line">        result_loss = loss(output,targets)</span><br><span class="line">        <span class="comment"># 把上一个循环中每个参数对应的梯度清零</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播，计算更新参数对应的梯度</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 参数调优</span></span><br><span class="line">        optim.step()</span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure><h1 id="使用网络模型调参"><a href="#使用网络模型调参" class="headerlink" title="使用网络模型调参"></a>使用网络模型调参</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data = torchvision.datasets.ImageNet(&quot;./dataset_ImageNet&quot;,split=&#x27;train&#x27;,transform=torchvision.transforms.ToTensor())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当为false的时候，加载网络模型——&gt;默认参数，可以将pretrained=False改为weight=none；为True的时候需要从网络中下载每一个参数pretrained=True改成weight=default</span></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(vgg16_true)</span></span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对VGG-16 True进行调参修改,加到最后</span></span><br><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line"><span class="comment"># print(vgg16_true)</span></span><br><span class="line"><span class="comment">#加到相应标签下</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line"><span class="comment"># print(vgg16_true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(vgg16_false)</span></span><br><span class="line"><span class="comment">#对某一个步骤进行修改</span></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] =nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure><h1 id="网络模型的保存与读取"><a href="#网络模型的保存与读取" class="headerlink" title="网络模型的保存与读取"></a>网络模型的保存与读取</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 保存方式1,保存网络模型的结构和参数</span></span><br><span class="line"><span class="comment"># 在别的文件使用方法 model = torch.load(&quot;vgg16_method1.pth&quot;)</span></span><br><span class="line">torch.save(vgg16,<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="comment"># 方式一陷阱</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">cnn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(cnn, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">Cnn = cnn()</span><br><span class="line">torch.save(cnn,<span class="string">&quot;Cnn_method1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载会报错，需要将模型的定义一并放到引用文件中，或者from model_save import *也可以</span></span><br><span class="line">model = torch.load(<span class="string">&quot;Cnn_method1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式2(推荐)</span></span><br><span class="line"><span class="comment"># 将vgg16的参数保存成字典型</span></span><br><span class="line"><span class="comment"># 在别的文件中使用方法 model = torch.load(&quot;vgg16_method2.pth&quot;)</span></span><br><span class="line">torch.save(vgg16.state_dict(),<span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方法</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>))</span><br></pre></td></tr></table></figure><h1 id="完整的模型训练套路"><a href="#完整的模型训练套路" class="headerlink" title="完整的模型训练套路"></a>完整的模型训练套路</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 字符串格式化写法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">cnn = CNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(cnn.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---------第&#123;&#125;轮训练开始---------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成训练模式，与测试模式一样，只有当网络中有Dropout层、BatchNorm层等的时候才发挥作用</span></span><br><span class="line">    cnn.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        outputs = cnn(imgs)</span><br><span class="line">        loss = loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器优化模型</span></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 参数优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step=total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="comment"># 加上item()会将tensor值转化为真实的数字，都可以</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成测试模式</span></span><br><span class="line">    cnn.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 整体正确的个数</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 没有梯度，方便调优</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets = data</span><br><span class="line">            outputs = cnn(imgs)</span><br><span class="line">            loss = loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss = total_test_loss+loss</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.save(cnn,&quot;cnn_&#123;&#125;.pth&quot;.format(i))</span></span><br><span class="line">    <span class="comment"># print(&quot;模型已保存&quot;)</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>上述的准确率计算的说明：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">outputs = torch.tensor([[<span class="number">0.1</span>,<span class="number">0.2</span>],</span><br><span class="line">                       [<span class="number">0.3</span>,<span class="number">0.4</span>]])</span><br><span class="line"><span class="comment"># 参数为1的话从左往右看，0的话从上往下看</span></span><br><span class="line"><span class="built_in">print</span>(outputs.argmax(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 看到预测的分类是哪个</span></span><br><span class="line">preds = outputs.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 实际的分类是哪个</span></span><br><span class="line">targets = torch.tensor([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算出对应位置相等的个数</span></span><br><span class="line"><span class="built_in">print</span>((preds == targets).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p>代码的CNN模型&lt;未调参&gt;：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.model(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数用来debug，查看参数等</span></span><br><span class="line"><span class="comment"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="comment">#     cnn =CNN()</span></span><br><span class="line"><span class="comment">#     input = torch.ones(64,3,32,32)</span></span><br><span class="line"><span class="comment">#     output = cnn(input)</span></span><br><span class="line"><span class="comment">#     print(output.shape)</span></span><br></pre></td></tr></table></figure><h1 id="如何使用GPU来训练"><a href="#如何使用GPU来训练" class="headerlink" title="如何使用GPU来训练"></a>如何使用GPU来训练</h1><p>第一种方式：需要网络模型、数据（输入，输出）、损失函数，调用<code>.cuda()</code>返回即可</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># from model import *</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 准备数据集,数据集没有cuda方法</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 字符串格式化写法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.model(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="comment"># 网络模型转移到cuda上面,判断cuda是否可用</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    cnn = cnn.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数的cuda</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(cnn.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---------第&#123;&#125;轮训练开始---------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成训练模式，与测试模式一样，只有当网络中有Dropout层、BatchNorm层等的时候才发挥作用</span></span><br><span class="line">    cnn.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        <span class="comment"># 对输入数据进行cuda</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            targets = targets.cuda()</span><br><span class="line">        outputs = cnn(imgs)</span><br><span class="line">        loss = loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器优化模型</span></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 参数优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step = total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="comment"># 加上item()会将tensor值转化为真实的数字，都可以</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;耗时：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成测试模式</span></span><br><span class="line">    cnn.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 整体正确的个数</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 没有梯度，方便调优</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets = data</span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line">            outputs = cnn(imgs)</span><br><span class="line">            loss = loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss = total_test_loss+loss</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.save(cnn,&quot;cnn_&#123;&#125;.pth&quot;.format(i))</span></span><br><span class="line">    <span class="comment"># print(&quot;模型已保存&quot;)</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>第二种GPU训练方式：需要网络模型、数据（输入，输出）、损失函数，调用<code>.to(device)</code>，这个<code>divcie = torch.device(&quot;cpu&quot;)</code>也可以是<code>device = torch.device(&quot;cuda&quot;)</code>，电脑有多个显卡的话，可以指定<code>device = torch.device(&quot;cuda:n&quot;)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># from model import *</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 定义训练设备，如果cuda可用，采用GPU，不可用的话采用CPU</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_avaliable() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 字符串格式化写法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.model(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="comment"># 网络模型转移到cuda上面,判断cuda是否可用</span></span><br><span class="line"></span><br><span class="line">cnn = cnn.cuda()</span><br><span class="line">cnn = cnn.to(device) <span class="comment"># 也可以不用赋值，直接cnn.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数的cuda</span></span><br><span class="line"></span><br><span class="line">loss_fn = loss_fn.cuda()</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(cnn.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---------第&#123;&#125;轮训练开始---------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成训练模式，与测试模式一样，只有当网络中有Dropout层、BatchNorm层等的时候才发挥作用</span></span><br><span class="line">    cnn.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        <span class="comment"># 对输入数据调用硬件</span></span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        outputs = cnn(imgs)</span><br><span class="line">        loss = loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器优化模型</span></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 参数优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step = total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="comment"># 加上item()会将tensor值转化为真实的数字，都可以</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;耗时：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    <span class="comment"># 设置成测试模式</span></span><br><span class="line">    cnn.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 整体正确的个数</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 没有梯度，方便调优</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets = data</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">            outputs = cnn(imgs)</span><br><span class="line">            loss = loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss = total_test_loss+loss</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.save(cnn,&quot;cnn_&#123;&#125;.pth&quot;.format(i))</span></span><br><span class="line">    <span class="comment"># print(&quot;模型已保存&quot;)</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="完整的模型验证（测试，demo）套路"><a href="#完整的模型验证（测试，demo）套路" class="headerlink" title="完整的模型验证（测试，demo）套路"></a>完整的模型验证（测试，demo）套路</h1><p>核心是利用已经训练好的模型，然后给它提供输入。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">image_path = <span class="string">&quot;./img/dog.jpg&quot;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="comment"># 因为png格式是4个通道，除了RGB三通道外，还有一个透明度通道</span></span><br><span class="line"><span class="comment"># 调用image = image.convert(&#x27;RGB&#x27;)，保留颜色通道，加上这一步之后，不管是png还是jpg图片都可以运行</span></span><br><span class="line">image = image.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)),</span><br><span class="line">                                            torchvision.transforms.ToTensor()])</span><br><span class="line">image = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.model=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.model(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为train文件里面我没有保存模型，所以这里我是没法运行的，要先运行train文件</span></span><br><span class="line"><span class="comment"># 采用GPU训练的模型，在CPU加载的过程中，要映射到CPU上</span></span><br><span class="line">model = torch.load(<span class="string">&quot;cnn_9.pth&quot;</span>,map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"><span class="comment"># model的输入要是4维的</span></span><br><span class="line">torch.reshape(image,(<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"><span class="comment"># 记得写上</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.argmax(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python</title>
      <link href="/2023/01/08/python/"/>
      <url>/2023/01/08/python/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p>首先记录pycharm的一些快捷键：</p><ul><li><code>shift</code>+<code>F10</code>：运行py文件</li><li><code>CTRL</code>：查看类或函数的使用方法</li><li><code>CTRL</code>+<code>/</code>进行注释</li><li><code>CTRL</code>+<code>P</code>查看当前函数所需要的参数</li><li><code>CTRL</code>+<code>D</code>复制上一行的代码</li><li><code>CTRL</code>+<code>C</code>退出程序</li></ul><h2 id="python的IO"><a href="#python的IO" class="headerlink" title="python的IO"></a>python的IO</h2><ul><li>输出：<code>print()</code>，例如<code>print(&#39;hello world&#39;)</code>，<code>print()</code>可以接受多个字符串，字符串之间用<code>,</code>隔开，就可以将字符串连成一串输出。<code>print()</code>会依次打印各个字符串，遇到<code>,</code>就会输出一个空格。</li><li>输入：<code>input()</code>，例如<code>name=input()</code>。<code>input()</code>有一个字符串参数，例如<code>name = input(&#39;please enter your name: &#39;)</code>，input的返回值类型是str，会首先输出字符串参数的内容。</li></ul><h2 id="python的数据类型"><a href="#python的数据类型" class="headerlink" title="python的数据类型"></a>python的数据类型</h2><ul><li><p>整数：为了清楚，可以在数字中间以<code>_</code>分隔，例如<code>10_000_000_000</code>和<code>10000000000</code>是一样的。</p></li><li><p>字符串：以<code>&#39;</code>或<code>&quot;</code>括起来的任意文本。如果<code>&#39;</code>括起来内部还有<code>&#39;</code>这种的话，就要加转义字符<code>\</code>。python可以用<code>r&#39;&#39;</code>表示<code>&#39;&#39;</code>内部的字符串不转义。</p></li><li><p>布尔值：只有<code>True</code>和<code>False</code>两种值。可以直接用<code>True</code>和<code>False</code>表示布尔值，也可以通过布尔运算计算出来。布尔值可以用<code>and</code>、<code>or</code>和<code>not</code>运算。</p></li><li><p>空值：空值用<code>None</code>表示，不能理解为<code>0</code>，<code>0</code>是有意义的，而<code>None</code>是一个特殊的空值。</p></li><li><p><strong>变量：python是动态语言，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量。</strong></p></li><li><p>常量：常用大写变量名表示常量，但是这个常量仍然是一个变量，因为python没有任何机制能够保证常量是常量，只是我们自己心里的一个默认罢了。</p></li><li><p><code>list</code>列表：<code>list</code>是一种有序的集合，可以随时添加和删除其中的元素。例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates = [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br></pre></td></tr></table></figure><ul><li><p>变量<code>classmates</code>就是一个list。用<code>len()</code>函数可以获得list元素的个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(classmates)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></li><li><p>用索引来访问list中每一个位置的元素，记得索引是从<code>0</code>开始的，最后一个元素的索引是<code>len(classmates) - 1</code>。如果要取最后一个元素，除了计算索引位置外，还可以用<code>-1</code>做索引，直接获取最后一个元素，以此类推，可以获取倒数第2个、倒数第3个。当索引超出了范围时，Python会报一个<code>IndexError</code>错误。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;Michael&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">1</span>]</span><br><span class="line"><span class="string">&#x27;Bob&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">2</span>]</span><br><span class="line"><span class="string">&#x27;Tracy&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">3</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">IndexError: <span class="built_in">list</span> index out of <span class="built_in">range</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[-<span class="number">1</span>]</span><br><span class="line"><span class="string">&#x27;Tracy&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[-<span class="number">2</span>]</span><br><span class="line"><span class="string">&#x27;Bob&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[-<span class="number">3</span>]</span><br><span class="line"><span class="string">&#x27;Michael&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[-<span class="number">4</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">IndexError: <span class="built_in">list</span> index out of <span class="built_in">range</span></span><br></pre></td></tr></table></figure></li><li><p>list是一个可变的有序表，所以，可以往list中追加元素到末尾。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.append(<span class="string">&#x27;Adam&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p>也可以把元素插入到指定的位置，比如索引号为<code>1</code>的位置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.insert(<span class="number">1</span>, <span class="string">&#x27;Jack&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Jack&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p>要删除list末尾的元素，用<code>pop()</code>方法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.pop()</span><br><span class="line"><span class="string">&#x27;Adam&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Jack&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p>要删除指定位置的元素，用<code>pop(i)</code>方法，其中<code>i</code>是索引位置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.pop(<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;Jack&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p>要把某个元素替换成别的元素，可以直接赋值给对应的索引位置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">1</span>] = <span class="string">&#x27;Sarah&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Sarah&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p>list里面的元素的数据类型也可以不同。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [<span class="string">&#x27;Apple&#x27;</span>, <span class="number">123</span>, <span class="literal">True</span>]</span><br></pre></td></tr></table></figure></li><li><p>list元素也可以是另一个list。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = [<span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;java&#x27;</span>, [<span class="string">&#x27;asp&#x27;</span>, <span class="string">&#x27;php&#x27;</span>], <span class="string">&#x27;scheme&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(s)</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><p>要注意<code>s</code>只有4个元素，其中<code>s[2]</code>又是一个list，如果拆开写就更容易理解了：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = [<span class="string">&#x27;asp&#x27;</span>, <span class="string">&#x27;php&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = [<span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;java&#x27;</span>, p, <span class="string">&#x27;scheme&#x27;</span>]</span><br></pre></td></tr></table></figure><p>要拿到<code>&#39;php&#39;</code>可以写<code>p[1]</code>或者<code>s[2][1]</code>，因此<code>s</code>可以看成是一个二维数组。</p></li><li><p>如果一个list中一个元素也没有，就是一个空的list，它的长度为0。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(L)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>tuple：另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改。</p><ul><li><pre><code class="python">&gt;&gt;&gt; classmates = (&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;)<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  classmates这个tuple不能变了，它也没有append()，insert()这样的方法。其他获取元素的方法和list是一样的，你可以正常地使用`classmates[0]`，`classmates[-1]`，但不能赋值成另外的元素。</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">- 当你定义一个tuple时，在定义的时候，tuple的元素就必须被确定下来。</span><br><span class="line"></span><br><span class="line">  ```python</span><br><span class="line">  &gt;&gt;&gt; t = (1, 2)</span><br><span class="line">  &gt;&gt;&gt; t</span><br><span class="line">  (1, 2)</span><br></pre></td></tr></table></figure></code></pre></li><li><p>如果要定义一个空的tuple，可以写成<code>()</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = ()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">()</span><br></pre></td></tr></table></figure></li><li><p>但是，要定义一个只有1个元素的tuple，如果这么定义:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>定义的不是tuple，是<code>1</code>这个数！这是因为括号<code>()</code>既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是<code>1</code>。所以，只有1个元素的tuple定义时必须加一个逗号<code>,</code>，来消除歧义:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">1</span>,)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">(<span class="number">1</span>,)</span><br></pre></td></tr></table></figure><p>Python在显示只有1个元素的tuple时，也会加一个逗号<code>,</code>。</p></li><li><p>一个”可变的“tuple：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">&#x27;X&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">2</span>][<span class="number">1</span>] = <span class="string">&#x27;Y&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, [<span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;Y&#x27;</span>])</span><br></pre></td></tr></table></figure></li><li><p>这个tuple定义的时候有3个元素，分别是<code>&#39;a&#39;</code>，<code>&#39;b&#39;</code>和一个list。</p></li></ul></li></ul><img src="/2023/01/08/python/tuple1.png" alt="tuple1" style="zoom:67%;"><p>当我们把list的元素<code>&#39;A&#39;</code>和<code>&#39;B&#39;</code>修改为<code>&#39;X&#39;</code>和<code>&#39;Y&#39;</code>后，tuple变为：</p><p>​<img src="/2023/01/08/python/tuple2.png" alt="tuple2" style="zoom:67%;"></p><p>表面上看，tuple的元素确实变了，但其实变的不是tuple的元素，而是list的元素。tuple一开始指向的list并没有改成别的list，所以，tuple所谓的“不变”是说，tuple的每个元素，指向永远不变。即向<code>&#39;a&#39;</code>，就不能改成指向<code>&#39;b&#39;</code>，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的。</p><p><strong>Python的整数没有大小限制，Python的浮点数也没有大小限制，但是超出一定范围就直接表示为<code>inf</code>（无限大）。</strong></p><ul><li>类型转换例如：int(str)</li></ul><h2 id="python的计算"><a href="#python的计算" class="headerlink" title="python的计算"></a>python的计算</h2><ul><li>一种除法是<code>/</code>，<code>/</code>计算结果是浮点数。</li><li>还有一种除法是<code>//</code>，两个整数的除法仍然是整数。</li><li>取余<code>%</code></li></ul><h2 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h2><p>python的字符串以Unicode编码，对于单个字符编码，<code>ord()</code>函数获取字符的整数表示，<code>chr()</code>函数把编码转换为对应的字符。python对<code>bytes</code>类型的数据用带b前缀的单引号或双引号表示，例如<code>x=b&#39;ABC&#39;</code>。以Unicode表示的<code>str</code>通过<code>encode()</code>方法可以编码为指定的<code>bytes</code>。要把<code>bytes</code>变为<code>str</code>，就需要用<code>decode()</code>方法。要计算<code>str</code>包含多少个字符，可以用<code>len()</code>函数，<code>len()</code>函数计算的是<code>str</code>的字符数，如果换成<code>bytes</code>，<code>len()</code>函数就计算字节数。</p><h2 id="格式化字符串"><a href="#格式化字符串" class="headerlink" title="格式化字符串"></a>格式化字符串</h2><p>python使用<code>%</code>格式化字符串，有几个<code>%?</code>占位符，后面就跟着几个变量或者值，如果只有一个<code>%?</code>，括号可以省略。例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;Hello, %s&#x27;</span> % <span class="string">&#x27;world&#x27;</span></span><br><span class="line"><span class="string">&#x27;Hi, %s, you have $%d.&#x27;</span> % (<span class="string">&#x27;Michael&#x27;</span>, <span class="number">1000000</span>)</span><br></pre></td></tr></table></figure><p>常见占位符：</p><table><thead><tr><th>占位符</th><th>替换内容</th></tr></thead><tbody><tr><td>%d</td><td>整数</td></tr><tr><td>%f</td><td>浮点数</td></tr><tr><td>%s</td><td>字符串</td></tr><tr><td>%x</td><td>十六进制整数</td></tr></tbody></table><p>如果你不太确定应该用什么，<code>%s</code>永远起作用，它会把任何数据类型转换为字符串。有些时候，字符串里面的<code>%</code>是一个普通字符怎么办？这个时候就需要转义，用<code>%%</code>来表示一个<code>%</code>。</p><p>另一种格式化字符串的方法是使用字符串的<code>format()</code>方法，它会用传入的参数依次替换字符串内的占位符<code>&#123;0&#125;</code>、<code>&#123;1&#125;</code>……，不过这种方式写起来比%要麻烦得多：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;小明&#x27;</span>, <span class="number">17.125</span>)</span><br><span class="line"><span class="string">&#x27;Hello, 小明, 成绩提升了 17.1%&#x27;</span></span><br></pre></td></tr></table></figure><p>最后一种格式化字符串的方法是使用以<code>f</code>开头的字符串，称之为<code>f-string</code>，它和普通字符串不同之处在于，字符串如果包含<code>&#123;xxx&#125;</code>，就会以对应的变量替换：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = <span class="number">2.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="number">3.14</span> * r ** <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="string">f&#x27;The area of a circle with radius <span class="subst">&#123;r&#125;</span> is <span class="subst">&#123;s:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">The area of a circle <span class="keyword">with</span> radius <span class="number">2.5</span> <span class="keyword">is</span> <span class="number">19.62</span></span><br></pre></td></tr></table></figure><p>上述代码中，<code>&#123;r&#125;</code>被变量<code>r</code>的值替换，<code>&#123;s:.2f&#125;</code>被变量<code>s</code>的值替换，并且<code>:</code>后面的<code>.2f</code>指定了格式化参数（即保留两位小数），因此，<code>&#123;s:.2f&#125;</code>的替换结果是<code>19.62</code>。</p><h2 id="If相关语句"><a href="#If相关语句" class="headerlink" title="If相关语句"></a>If相关语句</h2><ul><li><p>python中的if语句通过缩进表明后面的语句是同一个代码块的，这点和其他语言不同，并且要在条件之后加上一个冒号。例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">age = <span class="number">3</span></span><br><span class="line"><span class="keyword">if</span> age &gt;= <span class="number">18</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;your age is&#x27;</span>, age)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;adult&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;your age is&#x27;</span>, age)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;teenager&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>python中的else-if语句写法如，这里elif就是else if的缩写：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">age = <span class="number">3</span></span><br><span class="line"><span class="keyword">if</span> age &gt;= <span class="number">18</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;adult&#x27;</span>)</span><br><span class="line"><span class="keyword">elif</span> age &gt;= <span class="number">6</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;teenager&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;kid&#x27;</span>)</span><br></pre></td></tr></table></figure></li></ul><h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><p>python循环有两种。</p><ul><li><p>for…in循环，依次把list或tuple中的每个元素迭代出来.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">names = [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Tracy&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure><p><code>for x in...</code>循环就是把每个元素代入变量x，然后执行缩进块的语句，如下例子较为麻烦。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + x</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure><p>python有<code>range()</code>函数，可以生成整数序列，再通过<code>list()</code>函数可以转换为list</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>): <span class="comment">#这是从0-100，也可以写成list(range(101)),一种是列表，一种是序列</span></span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + x</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure></li><li><p>while循环</p><p>例如，计算100以内奇数之和：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">n = <span class="number">99</span></span><br><span class="line"><span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + n</span><br><span class="line">    n = n - <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure></li><li><p>break语句，提前结束循环，用法一致</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> n &lt;= <span class="number">100</span>:</span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">10</span>: <span class="comment"># 当n = 11时，条件满足，执行break语句</span></span><br><span class="line">        <span class="keyword">break</span> <span class="comment"># break语句会结束当前循环</span></span><br><span class="line">    <span class="built_in">print</span>(n)</span><br><span class="line">    n = n + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;END&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>continue语句，用法与其他语言一致</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> n &lt; <span class="number">10</span>:</span><br><span class="line">    n = n + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> n % <span class="number">2</span> == <span class="number">0</span>: <span class="comment"># 如果n是偶数，执行continue语句</span></span><br><span class="line">        <span class="keyword">continue</span> <span class="comment"># continue语句会直接继续下一轮循环，后续的print()语句不会执行</span></span><br><span class="line">    <span class="built_in">print</span>(n)</span><br></pre></td></tr></table></figure></li></ul><h2 id="dict和set"><a href="#dict和set" class="headerlink" title="dict和set"></a>dict和set</h2><p>字典dict其实就是map，是key-value的映射，特点是查找速度快。dict的key是不可变对象如字符串、整数，list是可变的</p><p>例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化，将数据放入dict</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">&#x27;Michael&#x27;</span>: <span class="number">95</span>, <span class="string">&#x27;Bob&#x27;</span>: <span class="number">75</span>, <span class="string">&#x27;Tracy&#x27;</span>: <span class="number">85</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">&#x27;Michael&#x27;</span>]</span><br><span class="line"><span class="number">95</span></span><br><span class="line"><span class="comment"># 通过key放入数据，一个key只能对应一个value</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">&#x27;Adam&#x27;</span>] = <span class="number">67</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">&#x27;Adam&#x27;</span>]</span><br><span class="line"><span class="number">67</span></span><br></pre></td></tr></table></figure><p>判断key是否存在</p><ul><li><p>通过<code>in</code>判断</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;Thomas&#x27;</span> <span class="keyword">in</span> d</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></li><li><p>通过dict提供的<code>get()</code>方法，如果key不存在，可以返回<code>None</code>，或者自己指定的value：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.get(<span class="string">&#x27;Thomas&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.get(<span class="string">&#x27;Thomas&#x27;</span>, -<span class="number">1</span>) <span class="comment"># 指定不存在的话value=-1</span></span><br><span class="line">-<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><p>删除key，用pop(key)方法，对应的value也会从dict中删除</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.pop(<span class="string">&#x27;Bob&#x27;</span>)</span><br><span class="line"><span class="number">75</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">&#x27;Michael&#x27;</span>: <span class="number">95</span>, <span class="string">&#x27;Tracy&#x27;</span>: <span class="number">85</span>&#125;</span><br></pre></td></tr></table></figure><p>set也是一组key的集合，但是不存储value，key不能重复，同样不能放入可变对象。</p><p>创建一个set，需要提供list作为输入集合。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="built_in">set</span>([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="built_in">set</span>([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125; <span class="comment">#重复元素会自动被过滤</span></span><br></pre></td></tr></table></figure><p>传入的参数<code>[1, 2, 3]</code>是一个list，而显示的<code>&#123;1, 2, 3&#125;</code>只是告诉你这个set内部有1，2，3这3个元素，显示的顺序也不表示set是有序的。</p><p>通过<code>add(key)</code>方法可以添加元素到set中，可以重复添加，但不会有效果；通过<code>remove(key)</code>方法可以删除元素；set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.add(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.add(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.remove(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 = <span class="built_in">set</span>([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s2 = <span class="built_in">set</span>([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 &amp; s2</span><br><span class="line">&#123;<span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 | s2</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br></pre></td></tr></table></figure><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1>]]></content>
      
      
      
        <tags>
            
            <tag> 语言 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA主成分分析</title>
      <link href="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
      <url>/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>PCA就是<strong>降维</strong>保存信息，例如将二维数据保留为一维数据。就是找到一个新的坐标系，这个坐标系的原点落在数据中心，坐标系的方向是往数据分布的方向走。我们需要保存的是新坐标系的原点、新坐标系的角度和新的坐标点。</p><p>PCA的目标就是要找到坐标系，使得保留了某些维度的时候，<strong>信息损失是最小的(信息保留最多)<strong>，比如说投影在某个轴上面，</strong>数据分布是最分散的</strong>。我们要找到数据分布最分散的方向(方差最大)，作为主成分(坐标轴)。新坐标系的第一个维度称为主成分一，第二个维度称为主成分二，如果我们找到数据在主成分一上面的投影分布方差是最大的时候，那么说明主成分一它能够保留最多的信息，在这个时候的坐标系就是最好的坐标系。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%9D%90%E6%A0%87%E8%BD%B4%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="坐标轴示意图"></p><p>如何实现PCA？</p><p>1.首先是要去中心化，就是把坐标原点放在数据的中心。方法就是把每一个值减去全部值的平均值。移动数据并不会改变数据点彼此之间的相对位置。</p><p>2.然后就是找坐标系(找到方差最大的方向) </p><p>我们该如何找到方差最大的方向呢？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2.png" alt="数据线性变换"></p><p>如果只是数据对角矩阵。用待变换矩阵左乘它就会得到<strong>伸缩</strong>之后的坐标，如图所示就是将x轴拉伸2倍长。原来是单位矩阵左乘，那么是让坐标不变，现在相应的扩大几倍。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A22.png" alt="数据线性变换2"></p><p>在矩阵运算上面可以想象到，这是逆时针旋转相应度数</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E8%BD%AC%E6%8D%A2.png" alt="转换"></p><p><strong>我们手上的数据就是我们要降维的数据</strong>。可以通过白数据左乘S再左乘R得到。</p><p>拉伸和旋转变换有什么作用？</p><p>拉伸决定了<strong>方差最大的方向</strong>是横或者纵向。旋转决定了方差最大的方向的角度是多大。<strong>所以我们要求的就是R矩阵</strong>。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E8%BD%AC%E6%8D%A22.png" alt="转换2"></p><p><strong>我们手上的数据</strong>D’同样也可以转换回白数据。可以先乘一个R的逆矩阵，它本来是逆时针旋转，取逆就是再顺时针旋转同样的度数。再拉伸，压缩回拉伸的倒数。</p><p>怎么求R？<strong>协方差矩阵的特征向量就是R</strong>。</p><p>何为协方差矩阵？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt="什么是协方差"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt="协方差矩阵"></p><p>白数据x,y不相关，这个时候协方差为0，只剩下对角线，即x、y方差为1就是一个单位矩阵。数据正相关协方差大于0，数据负相关，协方差小于0。</p><p>将协方差的公式代入协方差矩阵得到：再提个1&#x2F;(N-1)</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%BB%A3%E5%85%A5%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5.png" alt="代入协方差矩阵"></p><p>这里的D是移至原点后的<strong>原始数据矩阵</strong></p><p>我的想法是C既拉伸又旋转？</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%89%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8D%8F%E6%96%B9%E5%B7%AE.png" alt="手上数据的协方差"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F.png" alt="协方差的特征向量"></p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/R和L矩阵.png" alt="R和L矩阵" style="zoom: 80%;"><p>把特征值放到一起组成L矩阵，把特征向量v1和v2组合到一起成为R</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%80%BC.png" alt="协方差的特征值"></p><p>旋转回来协方差是L，因为R逆是一个对角矩阵，旋转回来之后x方向和y方向不相关，x方向的方差是a²，y方向的方差是b²，同时又是协方差矩阵的特征值。</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3PCA.png" alt="如何求解PCA"></p><p>如何判断PCA拟合度的高低？我们可以通过量化数据在主成分上的投影长度和最小或者数据在投影点到中心点的距离和最大，通常我们选择后者，因为后者更方便。</p><p>计算每个主成分的差异率就是把主成分分别的特征值除以全部的特征值加起来，越大越好，代表信息越多</p><p>主成分分析的应用</p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E4%B8%A4%E7%BB%84%E6%95%B0%E6%8D%AE.png" alt="两组数据"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E6%A0%B7%E6%9C%AC%E7%82%B9%E5%8E%BB%E4%B8%AD%E5%BF%83%E5%8C%96.png" alt="样本点去中心化"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E7%BB%93%E6%9E%9C.png" alt="结果"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E7%BB%98%E5%9B%BE.png" alt="绘图"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/%E5%8E%9F%E6%9C%89%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8C%96%E4%B8%BA%E4%B8%80%E7%BB%B4%E6%95%B0%E6%8D%AE.png" alt="原有数据转化为一维数据"></p><p><img src="/2023/01/05/PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/PCA%E6%AD%A5%E9%AA%A4.png" alt="PCA步骤"></p><p>主成分分析的本质就是向量换基。</p><p>主成分分析通过对投影距离方差的运用将降维问题转换成了求最值的问题。</p><p>主成分分为<strong>主成分分析和主成分评价</strong>两个方面，分析就是单纯的分析数据是否具有主成分和主成分效果如何，评价就是根据主成分运行的结果直接评价了。</p><p>在数模中<strong>主成分分析用于评价类的问题，即综合评估</strong></p><p>使用的<strong>注意要求</strong>是：指标之间的<strong>相关性比较高</strong>，一般需要对<strong>数据的相关性或者主成分分析的结果进行分析</strong>后，如果效果比较好，再使用主成分分析，如果效果不好，就不要使用主成分分析。</p><p>也就是说协方差越大，相关性越高，一般主成分分析相关性大部分变量协方差要大于0.3</p><p>主成分说白了就是在评价的时候有很多指标，因为指标太多了，并且各个指标之间相互有影响，为了消除指标之间的影响，单纯从数据的角度寻找各个指标具有公共特征，这些公共的特征就是主成分，也就是常说的第一主成分，第二主成分，第N主成分。具体的第一主成分第二主成分以及累计贡献率（要保证所有主成分累计对原始数据的贡献达到80%，即差异率超80%）是如何计算出的，可以不用掌握，且在比赛的时候也没必要写在论文上，只需要给出主要的结果即可。</p><p>主成分分析目的不是用来分类，而是用于综合评价</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 线性代数 </tag>
            
            <tag> 评价类模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Action</title>
      <link href="/2022/12/30/Action/"/>
      <url>/2022/12/30/Action/</url>
      
        <content type="html"><![CDATA[<h2 id="上传一个文件到博客的步骤如下："><a href="#上传一个文件到博客的步骤如下：" class="headerlink" title="上传一个文件到博客的步骤如下："></a>上传一个文件到博客的步骤如下：</h2><h3 id="清理缓存"><a href="#清理缓存" class="headerlink" title="清理缓存"></a>清理缓存</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><h3 id="生成新的静态文件"><a href="#生成新的静态文件" class="headerlink" title="生成新的静态文件"></a>生成新的静态文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><h3 id="本地预览博客"><a href="#本地预览博客" class="headerlink" title="本地预览博客"></a>本地预览博客</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><h3 id="上传到github仓库，将文件部署到博客上"><a href="#上传到github仓库，将文件部署到博客上" class="headerlink" title="上传到github仓库，将文件部署到博客上"></a>上传到github仓库，将文件部署到博客上</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><h2 id="其他的一些设置："><a href="#其他的一些设置：" class="headerlink" title="其他的一些设置："></a>其他的一些设置：</h2><h3 id="生成新的md文件"><a href="#生成新的md文件" class="headerlink" title="生成新的md文件"></a>生成新的md文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo n</span><br></pre></td></tr></table></figure><h2 id="butterfly设置："><a href="#butterfly设置：" class="headerlink" title="butterfly设置："></a>butterfly设置：</h2><h3 id="创建一个新的标签页"><a href="#创建一个新的标签页" class="headerlink" title="创建一个新的标签页"></a>创建一个新的标签页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page tags</span><br></pre></td></tr></table></figure><h3 id="创建一个分类页"><a href="#创建一个分类页" class="headerlink" title="创建一个分类页"></a>创建一个分类页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page categories</span><br></pre></td></tr></table></figure><h3 id="创建友情链接"><a href="#创建友情链接" class="headerlink" title="创建友情链接"></a>创建友情链接</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page <span class="built_in">link</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 技术操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/2022/12/30/CNN/"/>
      <url>/2022/12/30/CNN/</url>
      
        <content type="html"><![CDATA[<p><strong>卷积神经网络用处在哪？</strong></p><p>卷积神经网络是用于特征提取的。</p><p>传统的神经网络权重参数矩阵大、训练时间多、过拟合风险高。</p><p>可用于检测追踪任务、分类和检索、超分辨率重构、无人驾驶、人脸识别等</p><img src="/2022/12/30/CNN/超分辨率.png" alt="超分辨率" style="zoom:50%;"><p><strong>卷积神经网络(CNN)与传统网络(NN)的区别：</strong></p><img src="/2022/12/30/CNN/卷积神经网络与传统网络的区别.png" alt="卷积神经网络与传统网络的区别" style="zoom: 80%;"><ul><li>NN输入的是像素点，而CNN输入的是一张原始图像(h×w×c)，一个是一维，一个是三维。</li></ul><p><strong>卷积神经网络的整体架构：</strong></p><ul><li>输入层：输入一个图像数据</li><li>卷积层：尽可能多的提取特征</li><li>池化层：压缩、下采样特征</li><li>全连接层：通过一组权重参数，把输入层和输出层连在一起</li></ul><p><strong>卷积：</strong>卷积做了一件什么事？</p><img src="/2022/12/30/CNN/卷积做了一件什么事.png" alt="卷积做了一件什么事" style="zoom:60%;"><p>对不同的区域提取出不同的特征，将一张图像分成不同的部分，区别处理，进行图像分割。</p><p><strong>图像颜色通道</strong>：</p><p>R,G,B：要对三个颜色通道分别做计算，把三个通道卷积完的结果<strong>加</strong>在一起。对于每一个区域都要进行特征提取，得到最终的特征值。</p><img src="/2022/12/30/CNN/RGB特征提取.png" alt="RGB特征提取" style="zoom:67%;"><p>输入的图像维度c是多少，那么卷积核的维度c也应该是多少。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积核就是：每多大区域选出一个特征，一个区域对应出一个特征值。 相当于权重，刚开始是随机初始化的，然后学习更新。即下图W0</p><p>所有的卷积网络都是用<strong>内积</strong>做计算，<strong>对应位置相乘</strong>，所有结果加一起就可以了。结果别忘了加一个偏置项。在每一个卷积层中的特征矩阵w,h应该是相同的。在不通的卷积层中w,h可以不同</p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B.png" alt="卷积过程"></p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B2.png" alt="卷积过程2"></p><p><strong>卷积层涉及的参数</strong>：</p><ul><li><p>滑动窗口步长：注意<strong>步长</strong>为2得到的最终结果才是3*3，步长小，提取特征比较细致，效率慢；步长大，提取特征比较粗糙，特征少。<strong>一般对于图像而言，我们选择步长为1就可以</strong>，但是对于文本数据和其他数据步长不确定。</p></li><li><p>卷积核尺寸：卷积核越小，特征提取越细致，一般来说选<strong>3*3</strong></p></li><li><p>边缘填充：在边界外再加几圈0，能够弥补一些边界信息利用不充分问题。最外层只是扩充，因为是0，所以对最终结果不会产生影响。一般添一圈</p></li><li><p>卷积核个数：在算的过程当中要得到多少个特征图就有多少个卷积核。</p></li></ul><p>特征图的个数：特征图的个数取决于你给了多少份的权重矩阵，选择不通的权重矩阵，得到的特征图个数结果不一样。</p><p>卷积神经网络不止可以做一次卷积，一次可以提取出粗略特征，再卷积一次可以提取出中间特征，最后再提取出高级特征，再拿出高级特征来做分类。做一次卷积是不够的，需要做多次。</p><p><strong>卷积结果计算公式：</strong></p><img src="/2022/12/30/CNN/卷积结果计算公式.png" alt="卷积结果计算公式" style="zoom:80%;"><img src="/2022/12/30/CNN/卷积结果计算例子.png" alt="卷积结果计算例子" style="zoom: 33%;"><p><strong>卷积参数共享：</strong></p><p> 对于图中的每个区域都选择同样的卷积核，卷积核这个权值矩阵是不变的</p><p><img src="/2022/12/30/CNN/%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB.png" alt="参数共享"></p><p>每次卷积完都要加一个RELU函数，即非线性变换</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>在原始得到的特征上进行一个筛选，并不会改变特征图的个数。不涉及矩阵计算，只涉及筛选。</p><p><strong>最大池化</strong>：</p><p>选择不同区域，在每个区域中选择最大值，选择一个最大值说明这个特征比较重要。</p><img src="/2022/12/30/CNN/最大池化.png" alt="最大池化" style="zoom: 67%;"><p><strong>平均池化</strong>：</p><p>选择不同区域，在每个区域中求平均值。</p><p>我们<strong>选择最大池化</strong>，因为神经网络是个优胜劣汰的过程，我们选择最好的特征，不平均来把不好的特征拷进去。</p><p>两次卷积后一次池化，RELU是激活函数</p><p><img src="/2022/12/30/CNN/%E5%8D%B7%E7%A7%AF%E6%B1%A0%E5%8C%96.png" alt="卷积池化"></p><p>卷积和池化只是做特征提取的，到最后池化后会形成立体的特征图，对特征图进行分类，如何转化为分类的概率值？全连接层无法连三维的东西，我们需要将三维的特征图拉长形成特征向量，全连接层如果是五分类，池化层得到的特征图大小为<code>32*32*10</code>，那么得到的全连接层是[10240,5]，相当于将10240个特征转化为我们预测的五个类别的概率值。所以在Pooling层和FC层之间还有一个拉长的操作（转换操作）</p><p><strong>什么才能称为一层</strong>？带参数计算的才能被称为一层。卷积层带，RELU层(激活层)不带参数计算，池化层不带参数计算，不更新参数之类的，全连接层也有权重参数矩阵，需要更新参数。</p><img src="/2022/12/30/CNN/特征图的变化.png" alt="特征图的变化" style="zoom:67%;"><p><strong>感受野：</strong></p><img src="/2022/12/30/CNN/感受野.png" alt="感受野" style="zoom:60%;"><p>我们希望感受野越大越好。</p><img src="/2022/12/30/CNN/感受野2.png" alt="感受野2" style="zoom:60%;">]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学建模 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
